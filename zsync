#!/usr/bin/env python3

version = "3.0.1"

# TODO: cleanup within refresh function
# does canceling a transfer (refresh) leave garbage snapshots on target?

# TODO: implement resuming


import argparse
import fnmatch
import subprocess
import sys
import time
from datetime import datetime
from shutil import which

usage = f"""
zsync version: {version}
Uses ZFS send/receive to sync a ZFS filesystem between source and destination\n

Usage: zsync [OPTION] <source dataset> <destination dataset>
 or    zsync <source dataset> <destination dataset>
 or    zsync <source dataset> [<USER>@]<HOST>:<destination dataset>
 or    zsync [<USER>@]<HOST>:<source dataset> <destination dataset>
"""


bookmark_suffix = "zsync_incremental"
quiet = not sys.stdout.isatty()



def main():
    parser = argparse.ArgumentParser(description=usage, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("source_dataset")
    parser.add_argument("destination_dataset")

    parser.add_argument(
        "-v",
        dest="verbose",
        action="store_true",
        help="Enable verbose output"
    )
    parser.add_argument(
        "-f",
        "--full",
        "--force",
        dest="force",
        action="store_true",
        help="Perform a full backup if differential information is missing"
        + " for destination (overwrites existing data on destination)",
    )
    parser.add_argument(
        "-s",
        "--ssh",
        dest="ssh",
        action="store_true",
        help="Send to or receive from a remove ssh host",
    )
    parser.add_argument(
        "--version",
        dest="version_query",
        action="version",
        version=f"zsync version: {version}",
        help="Print zsync version",
    )

    args = parser.parse_args()

    global verbose
    verbose = args.verbose
    global force
    force = args.force
    global ssh
    ssh = args.ssh

    src = Source(args.source_dataset)
    dest = Destination(args.destination_dataset)

    check_req_apps(src, dest, ["zfs"])

    for pool, dest_path in dest.pool_manager():
        snap_name = src.snapshot()
        success = sync(src, dest, pool, dest_path, snap_name)
        if success:
            vlog("Sync Succeeded")
            src.bookmark(snap_name, dest, dest_path)
        else:
            vlog("Sync Failed")
            src.delete_snapshot(snap_name)



def check_req_apps(source, destination, required_apps):
    # Check that required programs exist
    for app in required_apps:
        if not (source.ssh and destination.ssh):
            if which(app) is None:
                elog(f"{app}: command not found.")
                exit(2)
        if source.ssh:
            if not execute(f"{source.ssh_cmd} which {app}"):
                elog(f"on {source.remote_host}: {app}: command not found.")
                exit(2)
        if destination.ssh:
            if not execute(f"{destination.ssh_cmd} which {app}"):
                elog(f"on {destination.remote_host}: {app}: command not found.")
                exit(2)


def sync(source, destination, dest_pool, dest_path, snap_name):
    formatted_dest = dest_path.replace("/", "_")
    if destination.ssh:
        undated_bookmark_name = f"{source.zfs_path}#{destination.remote_host}:{formatted_dest}"
    else:
        undated_bookmark_name = f"{source.zfs_path}#{formatted_dest}"

    log(f"Syncing destination '{dest_path}'")
    dest_dataset_exists = dest_path in execute(f"{destination.ssh_cmd} zfs list -r {dest_pool} -o name")

    # find bookmark on source that matches destination
    matching_bookmark = None
    matching_dest_snapshot = None
    if bookmarks := list_bookmarks(source.ssh_cmd, source.zfs_path):
        if matching_bookmarks := [b for b in bookmarks if undated_bookmark_name in b]:
            # select newest bookmark and delete all duplicates
            matching_bookmark = matching_bookmarks[0]
            vlog(f"Found matching bookmark: {matching_bookmark}")
            for old_bookmark in matching_bookmarks[1:]:
                vlog(f"Destroying old bookmark: {old_bookmark}")
                execute(f"zfs destroy {old_bookmark}")

            # find snapshot on destination with same datetime as matching bookmark
            bookmark_datetime = matching_bookmark.split("::")[1]
            snapshots = list_snapshots(destination.ssh_cmd, dest_path)
            if matching_dest_snapshots := [s for s in snapshots if bookmark_datetime in s]:
                matching_dest_snapshot = matching_dest_snapshots[0]
                vlog(f"Found matching snapshot on destination: {matching_dest_snapshot}")
                if snapshots[0] != matching_dest_snapshot:
                    vlog("There are more recent snapshots on destination. Deleting")
                    for snapshot in snapshots:
                        if snapshot == matching_dest_snapshot:
                            break
                        source.delete_snapshot(snapshot)
            else:
                vlog(f"Could not find snapshot on destination with datetime of {bookmark_datetime}")
    else:
        ssh_msg = f" on {source.remote_host}" if source.ssh else ""
        vlog(f"No bookmarks found for {source.zfs_path}{ssh_msg}")

    try:
        if dest_dataset_exists:
            if matching_bookmark and matching_dest_snapshot:
                return refresh(source, snap_name, matching_bookmark, destination, dest_path)
            else:
                if not force:
                    elog(f"destination dataset '{dest_path}' exists but no differential information "
                         "is available (bookmark missing).\nSupply -f to force overwrite")
                    return False
                else:
                    log(f"Destination dataset '{dest_path}' exists but no differential information "
                        "is available (bookmark missing)\nWill overwrite")
                    vlog(f"Destroying destination dataset '{dest_path}'")
                    execute(f"{destination.ssh_cmd} zfs destroy -r {dest_path}", return_rc=True)
                    return init(source, snap_name, destination, dest_path)
        else:
            if matching_bookmark:
                log(f"Bookmark '{matching_bookmark}' exists but its dataset is missing")
                log("Destroying bookmark")
                execute(f"{source.ssh_cmd} zfs destroy {matching_bookmark}")

            return init(source, snap_name, destination, dest_path)

    except KeyboardInterrupt:
        log("\nTerminating zsync")
        log("Cleaning up")
        raise SystemExit



def init(source, snap_name, destination, dest_path):
    log(f"Performing full backup of {source.zfs_path}")

    log(f"Sending '{snap_name}' to '{dest_path}'")
    success = send_recv(source, snap_name, None, destination, dest_path)

    if not success:
        elog("ZFS send failed")
    return success


def refresh(source, snap_name, bookmark_name, destination, dest_path):
    log(f"Sending incremental snapshot '{snap_name}' to '{dest_path}'")
    success = send_recv(source, snap_name, bookmark_name, destination, dest_path)

    # if that didn't work, try full backup
    if not success:
        elog("ZFS incremental send failed")
        if force:
            log("Attempting full backup")
            vlog(f"Destroying destination dataset '{dest_path}'")
            execute(f"{destination.ssh_cmd} zfs destroy -r {dest_path}")
            return init(source, snap_name, destination, dest_path)
        else:
            log("Supply -f to force overwrite (full backup)")
            return False

    # only save most recent snapshot on destination
    purgelist = list_snapshots(destination.ssh_cmd, dest_path)[1:]
    if purgelist:
        for snap in purgelist:  # delete snapshots remaining on list
            vlog(f"Destroying snapshot on destination: {snap}")
            execute(f"{destination.ssh_cmd} zfs destroy {snap}")
    return True


def execute(command, return_rc=False):
    cmdarr = command.split()
    result = subprocess.run(cmdarr, text=True, capture_output=True)

    if return_rc:
        return result.returncode, result.stderr.strip()
    else:
        return result.stdout


def send_recv(source, snap_name, bookmark_name, destination, dest_path):
    if pvmissing := which("pv") is None:
        elog("pv: command not found. Cannot print status")

    # if encryption is enabled on destination
    parent_dataset = dest_path.rsplit("/", 1)[0]
    dest_encrypted = destination.properties[parent_dataset]["encryption"] != "off"
    src_encrypted = source.properties["encryption"] != "off"

    if dest_encrypted and not src_encrypted:
        send_opts = "Lc"
    else:
        send_opts = "w"

    if bookmark_name is not None:
        cmd1 = f"{source.ssh_cmd} zfs send -{send_opts} -i {bookmark_name} {snap_name}".split()
        cmd2 = f"{destination.ssh_cmd} zfs recv -vFu {dest_path}".split()
    else:
        cmd1 = f"{source.ssh_cmd} zfs send -{send_opts} {snap_name}".split()
        cmd2 = f"{destination.ssh_cmd} zfs recv -vFu {dest_path}".split()

    p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE)

    if not quiet and not pvmissing:
        p2 = subprocess.Popen(
            ["pv", "-f"],
            stdin=p1.stdout,
            stdout=subprocess.PIPE,
            stderr=sys.stdout,
        )  # universal_newlines=True,
    else:
        p2 = p1

    # this one doesn't like to die even when ran in native bash shell
    p3 = subprocess.Popen(cmd2, stdin=p2.stdout, stdout=subprocess.PIPE)

    p3.wait()  # wait on p3 because p1 may never return in the event of recv failure

    if p3.returncode != 0:
        vlog(f"Process returncode: {p3.returncode}")

    return p3.returncode == 0


def list_snapshots(ssh_cmd, dataset):
    snaplist = execute(f"{ssh_cmd} zfs list -t snapshot -s creation -o name {dataset}").splitlines()
    if snaplist:
        del snaplist[0]  # remove header from list
        snaplist.reverse()  # most recent first
    return snaplist


def list_bookmarks(ssh_cmd, dataset):
    bookmarklist = execute(f"{ssh_cmd} zfs list -t bookmark -s creation -o name {dataset}").splitlines()
    if bookmarklist:
        del bookmarklist[0]  # remove header from list
        bookmarklist.reverse()  # most recent first
    return bookmarklist


def log(message):
    if not quiet:
        print(message)


def vlog(message):
    if verbose and not quiet:
        print(message)


def elog(message):
    date_time = datetime.now().isoformat(timespec='seconds')
    print(f"{date_time} | Error: {message}")



class Path:
    def __init__(self, path):
        self.zfs_path = path
        self.ssh_cmd = ""
        self.remote_user = "root"
        self.remote_host = ""
        self.ssh = self.parse_ssh()

    def parse_ssh(self):
        if ":" in self.zfs_path:
            self.remote, self.zfs_path = self.zfs_path.split(":")
            if "@" in self.remote:
                self.remote_user, self.remote_host = self.remote.split("@")
            else:
                self.remote_host = self.remote
            self.ssh_cmd = f"ssh {self.remote}"
            # check that ssh credential will work without password
            vlog("Checking ssh credentials")
            test_cmd = f"ssh -o PasswordAuthentication=no -o BatchMode=yes {self.remote} exit"
            resp = subprocess.run(test_cmd.split(), text=True, capture_output=True)
            if resp.stderr:
                elog(f" {resp.stderr}")
                log("Cannot connect to remote host. exiting.")
                quit()

            return True
        else:
            return False

    def permissions(self, path):
        vlog(f"Checking permissions on dataset: {self.remote_host}{':' if self.ssh else ''}{self.zfs_path}")
        raw_perms = execute(f"{self.ssh_cmd} zfs allow {path}")
        if raw_perms:
            for line in raw_perms.splitlines()[2:]:
                if f"user {self.remote_user}" in line:
                    return line.split(f"user {self.remote_user}")[1].strip().split(",")
        else:
            return []

    def get_properties(self, path):
        vlog(f"Checking properties on dataset: {self.remote_host}{':' if self.ssh else ''}{path}")
        properties = {}
        raw_props = execute(f"{self.ssh_cmd} zfs get all {path}").splitlines()
        for line in raw_props[1:]:
            line = line.split()
            properties[line[1]] = line[2]
        return properties


class Source(Path):
    def __init__(self, path):
        Path.__init__(self, path)
        self.check_source()
        self.properties = self.get_properties(self.zfs_path)


    def check_source(self):
        # check that source dataset exists
        if self.ssh:
            if self.zfs_path not in execute(f"{self.ssh_cmd} zfs list"):
                elog(f"on {self.remote_host}: source dataset '{self.zfs_path}' does not exist")
                exit(2)

            # Check permissions of remote user
            if not self.remote_user == "root":
                perms = self.permissions(self.zfs_path)
                if missing_perms := set(["snapshot", "bookmark", "send"]).difference(perms):  #TODO: determine correct source perms
                    elog(f"user '{self.remote_user}' on remote source '{self.remote_host}:{self.zfs_path}'"
                         f" lacks required permissions: {missing_perms}")
                    exit(2)
        else:
            if self.zfs_path not in execute("zfs list"):
                elog(f"source dataset '{self.zfs_path}' does not exist")
                exit(2)


    def snapshot(self):
        date_time = datetime.now().isoformat(timespec='seconds')
        snap_name = f"{self.zfs_path}@zsync::{date_time}"
        if self.ssh:
            vlog(f"Snapshotting source '{snap_name}' on {self.remote_host}")
            execute(f"{self.ssh_cmd} zfs snapshot {snap_name}")
        else:
            vlog(f"Snapshotting source '{snap_name}'")
            execute(f"zfs snapshot {snap_name}")
        return snap_name


    def bookmark(self, snap_name, destination, dest_path):
        date_time = snap_name.split("::")[1]
        formatted_dest = dest_path.replace("/", "_")
        if destination.ssh:
            undated_bookmark = f"{self.zfs_path}#{destination.remote_host}:{formatted_dest}"
        else:
            undated_bookmark = f"{self.zfs_path}#{formatted_dest}"

        if existing_bookmarks := list_bookmarks(self.ssh_cmd, self.zfs_path):
            vlog("Deleting old bookmarks")
            matching_bookmarks = [b for b in existing_bookmarks if undated_bookmark in b]
            for bookmark in matching_bookmarks:
                vlog(f"Destroying bookmark '{bookmark}'")
                execute(f"{self.ssh_cmd} zfs destroy {bookmark}")

        bookmark_name = f"{undated_bookmark}::{date_time}"
        vlog(f"Converting source snapshot into bookmark: {snap_name} -> {bookmark_name}")
        execute(f"{self.ssh_cmd} zfs bookmark {snap_name} {bookmark_name}")
        self.delete_snapshot(snap_name)


    def delete_snapshot(self, snap_name):
        vlog(f"Deleting snapshot '{snap_name}'")
        for _ in range(3):
            rc, stderr = execute(f"{self.ssh_cmd} zfs destroy {snap_name}", return_rc=True)
            if rc == 0:
                return
            time.sleep(1)
        elog(f"could not delete snapshot '{snap_name}' : {stderr}")



class Destination(Path):
    def __init__(self, path):
        Path.__init__(self, path)
        self.check_wildcards()
        self.check_destination()
        self.properties = {}
        for pool, dest_path in self.pool_manager(no_mount=True):
            parent_dataset = dest_path.rsplit("/", 1)[0]
            self.properties[parent_dataset] = self.get_properties(parent_dataset)


    def check_wildcards(self):
        """Finds every pools that matches destination.zfs_path with wildcard"""
        vlog("Querying ZFS pools available for import. This may take a few seconds.")
        imported = [pool for pool in execute(f"{self.ssh_cmd} zpool list -o name").splitlines() if pool != "NAME"]
        exported = [pool.split(": ")[1] for pool in execute(f"{self.ssh_cmd} zpool import").splitlines() if "pool:" in pool]
        dest_pool, self.dataset_path = self.zfs_path.split("/", 1)

        self.imported_pools = []
        self.exported_pools = []
        if "*" in dest_pool or "?" in dest_pool:
            vlog("Checking wildcards")
            self.imported_pools = fnmatch.filter(imported, dest_pool)
            self.exported_pools = fnmatch.filter(exported, dest_pool)
            if not (self.imported_pools or self.exported_pools):
                elog(f"no valid substitutions for '{dest_pool}' exist")
                exit(2)
        else:  # not checking wildcard
            if dest_pool in imported:
                self.imported_pools = [dest_pool]
            elif dest_pool in exported:
                self.exported_pools = [dest_pool]
            else:
                elog(f"destination pool '{dest_pool}' does not exist")
                exit(2)

        remote_string = f" on remote host '{self.remote_host}'" if self.ssh else ""
        vlog(f"Syncing to pools: {self.imported_pools + self.exported_pools}{remote_string}")


    def check_destination(self):
        if self.ssh and not self.remote_user == "root":
            for pool, dest_path in self.pool_manager(no_mount=True):
                perms = self.permissions(dest_path)
                if missing_perms := set(["create", "destroy", "mount", "receive"]).difference(perms):
                    elog(f"user '{self.remote_user}' on remote destination '{self.remote_host}:{dest_path}'"
                         f" lacks required permissions: {missing_perms}")
                    exit(2)



    def pool_manager(self, no_mount=False):
        for pool in self.exported_pools:
            try:
                self.import_pool(pool, no_mount)
                yield pool, f"{pool}/{self.dataset_path}"
            except KeyboardInterrupt:
                log("\nReceived interrupt while importing pool")
                log("Cleaning up")
            finally:
                retries = 2
                while self.export_pool(pool) == 1 and retries > 0:
                    print("Retrying...")
                    retries -= 1

        for pool in self.imported_pools:
            yield pool, f"{pool}/{self.dataset_path}"


    def import_pool(self, pool, no_mount):
        opt = "-N" if no_mount else ""
        vlog(f"Importing {opt} pool '{pool}'")
        rc, stderr = execute(f"{self.ssh_cmd} zpool import {opt} {pool}", return_rc=True)
        if rc != 0:
            elog(f"Error importing pool: '{pool}' - {stderr}")
            return rc


    def export_pool(self, pool):
        rc, stderr = execute(f"{self.ssh_cmd} zpool export {pool}", return_rc=True)
        if rc != 0:
            elog(f"Error exporting pool: '{pool}' - {stderr}")
            if "no such pool" in stderr:
                return 2
            else:
                return rc



if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log("Terminating zsync")
